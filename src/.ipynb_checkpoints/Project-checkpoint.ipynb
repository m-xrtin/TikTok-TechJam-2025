{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ba38d3-75e8-48a2-abc9-fe9831be5970",
   "metadata": {},
   "source": [
    "# Hello! ðŸ‘‹  \n",
    "Welcome to **Compile & Conquer's** workspace.\n",
    "\n",
    "Our project utilizes a combination of libraries and APIs to tackle the problem of spam review detection.  \n",
    "Below is a brief overview of our pipeline:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Training vs Inference\n",
    "\n",
    "ðŸ‘‰ **Training Pipeline (done by us):**\n",
    "- Standardize raw datasets into workable CSV format\n",
    "- Preprocess and clean the data\n",
    "- Use both OpenAI and VADER for pseudolabelling\n",
    "- Train CatBoost with 10-fold cross-validation\n",
    "- Save models + metadata\n",
    "\n",
    "ðŸ‘‰ **Inference Pipeline (for judges / new data):**\n",
    "- Standardize and preprocess new input\n",
    "- Add VADER sentiment for extra accuracy\n",
    "- Load pre-trained CatBoost models\n",
    "- Predict spam vs ham  \n",
    "- Save results to output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a77c66c2-8902-4a63-a021-26ce9def6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import random\n",
    "from parse_file import *\n",
    "from ucsd_json_standardization import *\n",
    "from standardization import *\n",
    "from helpers import *\n",
    "from preprocess import *\n",
    "from vader_function import *\n",
    "from inference import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11aea42-5cca-441e-9fa8-c3ca0a6a174f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We need to set some directories so that our program can access all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a2f584b-98ff-499b-b64e-144506d272af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "INPUT_FOLDER = os.path.join(PROJECT_ROOT, \"input\")\n",
    "DATA_FOLDER = os.path.join(PROJECT_ROOT, \"data\")\n",
    "OUTPUT_FOLDER = os.path.join(PROJECT_ROOT, \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ac5330e-ccee-4dea-b915-5a1c8b67a158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is successfully standardized!\n"
     ]
    }
   ],
   "source": [
    "input_file = \"test.json\" # Feel free to change! Though have to have the file of interest in the input folder of the project directory.\n",
    "output_file = standardize_file(input_file)\n",
    "output_path = os.path.join(DATA_FOLDER, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c490d-5bbd-4930-9d19-0b66a318f6f5",
   "metadata": {},
   "source": [
    "Feel free to take a look at the standardization, but we will continue now in the preprocessing stage.\n",
    "Our project has two approaches towards standardization, depending on the file we receive.\n",
    "Since the json from the UCSD Google Review dataset is so well formatted, we have a faster way of dealing with it.\n",
    "However, because we want scalibility and understand not every dataset is unfortunately treated the same, we also impletment OpenAI in helping us standardize, ALbeit trading some runtime..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "623d87c1-ef5a-41e2-9dd7-c9498e40e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Converted 10 rows into C:\\Users\\hanya\\Desktop\\TikTok TechJam\\TikTok-TechJam-2025\\data\\test_standardized.csv\n",
      "âœ… Preprocessing complete. Saved to C:\\Users\\hanya\\Desktop\\TikTok TechJam\\TikTok-TechJam-2025\\data\\test_standardized_preprocessed.csv\n",
      "Final columns: ['business_name', 'user_name', 'time', 'user_id', 'text', 'rating', 'sentiment_category', 'gmap_id']\n",
      "âœ… VADER sentiment scoring complete. Saved to C:\\Users\\hanya\\Desktop\\TikTok TechJam\\TikTok-TechJam-2025\\data\\test_standardized_final.csv\n"
     ]
    }
   ],
   "source": [
    "base_name = os.path.splitext(output_file)[0]  # standardized base name\n",
    "csv_path = os.path.join(DATA_FOLDER, base_name + \".csv\")\n",
    "json_to_csv_from_data(output_path, csv_path)\n",
    "_, preprocessed_csv = preprocess_file(base_name)\n",
    "_, final_csv = VADER_Sentiment_Score(base_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1392f1-7f4b-4d1c-8bfd-f46b87602cf1",
   "metadata": {},
   "source": [
    "Now our file is ready to be plugged into the model!!\n",
    "In this previous preprocessing stage, we get rid of potentially problematic rows, such as duplicate entries, or non-sensible values.\n",
    "We then add vader sentiment to increase the number of attributes used for training, which is hopefully going to increase the applicability and accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "249d3ba8-8088-47ae-90cc-dd79e22f2627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model\\\\features.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Final Step!\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m _, results_csv = \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\TikTok TechJam\\TikTok-TechJam-2025\\src\\inference.py:22\u001b[39m, in \u001b[36mrun_inference\u001b[39m\u001b[34m(base_name)\u001b[39m\n\u001b[32m     19\u001b[39m # Load input data\n\u001b[32m     20\u001b[39m df = pd.read_csv(input_path)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m # Load metadata\n\u001b[32m     23\u001b[39m feature_order = joblib.load(os.path.join(MODEL_FOLDER, \"features.pkl\"))\n\u001b[32m     24\u001b[39m cat_features = joblib.load(os.path.join(MODEL_FOLDER, \"cat_features.pkl\"))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\joblib\\numpy_pickle.py:650\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode)\u001b[39m\n\u001b[32m    648\u001b[39m         obj = _unpickle(fobj)\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    651\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[32m    652\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    653\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    654\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    655\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model\\\\features.pkl'"
     ]
    }
   ],
   "source": [
    "# Final Step!\n",
    "_, results_csv = run_inference(base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e0e87-e785-47b7-833d-4dcdb9347e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
